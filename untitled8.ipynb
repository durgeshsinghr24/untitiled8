{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt3ugN8orost"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
        "def preprocess(sent):\n",
        "    sent = nltk.word_tokenize(sent)\n",
        "    sent = nltk.pos_tag(sent)\n",
        "    return sent\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sent = preprocess(ex)\n",
        "sent\n",
        "\n",
        "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "cp = nltk.RegexpParser(pattern)\n",
        "cs = cp.parse(sent)\n",
        "print(cs)\n",
        "\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from pprint import pprint\n",
        "iob_tagged = tree2conlltags(cs)\n",
        "pprint(iob_tagged)\n",
        "\n",
        "from nltk import ne_chunk\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "ne_tree = ne_chunk(pos_tag(word_tokenize(ex)))\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
        "pprint([(X.text, X.label_) for X in doc.ents])\n",
        "\n",
        "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "def url_to_string(url):\n",
        "    res = requests.get(url)\n",
        "    html = res.text\n",
        "    soup = BeautifulSoup(html, 'html5lib')\n",
        "    for script in soup([\"script\", \"style\", 'aside']):\n",
        "        script.extract()\n",
        "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
        "ny_bb = url_to_string('https://www.nytimes.com/')\n",
        "#https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')\n",
        "article = nlp(ny_bb)\n",
        "len(article.ents)\n",
        "\n",
        "labels = [x.label_ for x in article.ents]\n",
        "Counter(labels)\n",
        "\n",
        "items = [x.text for x in article.ents]\n",
        "Counter(items).most_common(4)\n",
        "\n",
        "sentences = [x for x in article.sents]\n",
        "print(sentences[40])\n",
        "\n",
        "displacy.render(nlp(str(sentences[40])), jupyter=True, style='ent')\n",
        "\n",
        "displacy.render(nlp(str(sentences[20])), style='dep', jupyter = True, options = {'distance': 80})\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "sample_text=\"\"\"\n",
        "Rama killed Ravana to save Sita from Lanka.The legend of the Ramayan is the most popular Indian epic.A lot of movies and serials have already\n",
        "been shot in several languages here in India based on the Ramayana.\n",
        "\"\"\"\n",
        "tokenized=nltk.sent_tokenize(sample_text)\n",
        "for i in tokenized:\n",
        "  words=nltk.word_tokenize(i)\n",
        "  # print(words)\n",
        "  tagged_words=nltk.pos_tag(words)\n",
        "  # print(tagged_words)\n",
        "  chunkGram=r\"\"\"VB: {}\"\"\"\n",
        "  chunkParser=nltk.RegexpParser(chunkGram)\n",
        "  chunked=chunkParser.parse(tagged_words)\n",
        "\n",
        "displacy.render(nlp(str(sample_text)), style='dep', jupyter = True, options = {'distance': 80})\n",
        "\n",
        "import nltk\n",
        "sample_text= \"I am a coding ninja, and I am the best in coding.\"\n",
        "\n",
        "\n",
        "tokenized=nltk.sent_tokenize(sample_text)\n",
        "for i in tokenized:\n",
        "  words=nltk.word_tokenize(i)\n",
        "  tagged_words=nltk.pos_tag(words)\n",
        "  print(tagged_words)\n",
        "  chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" # this is the grammar that we define,\n",
        "  chunkParser=nltk.RegexpParser(chunkGram)\n",
        "  chunked=chunkParser.parse(tagged_words)\n",
        "\n",
        "displacy.render(nlp(str(sample_text)), style='dep', jupyter = True, options = {'distance': 100})\n",
        "\n",
        "import nltk\n",
        "sentence = [\n",
        "   (\"the\", \"DT\"),\n",
        "   (\"book\", \"NN\"),\n",
        "   (\"has\",\"VBZ\"),\n",
        "   (\"many\",\"JJ\"),\n",
        "   (\"chapters\",\"NNS\")\n",
        "]\n",
        "chunker = nltk.RegexpParser(\n",
        "   r'''\n",
        "   NP:{<DT><NN.*><.*>*<NN.*>}\n",
        "   }<VB.*>{\n",
        "   '''\n",
        ")\n",
        "chunker.parse(sentence)\n",
        "Output = chunker.parse(sentence)\n",
        "displacy.render(nlp(str(Output)), style='dep', jupyter = True, options = {'distance': 100})"
      ]
    }
  ]
}